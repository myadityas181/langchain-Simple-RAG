{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple RAG  USing our Custom Embedding Class\n",
    "USed openserach as vector DB.\n",
    "Tried with different Chain mathod like. Stuff and MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Required Loader from langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import (\n",
    "    UnstructuredURLLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    ConfluenceLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data from URL uding URLLoader\n",
    "loaderurl = UnstructuredURLLoader(\n",
    "    urls = [\n",
    "        \"https://www.who.int/westernpacific/news-room/feature-stories/item/4-things-to-know-about-covid-19-treatment-in-the-pacific\"\n",
    "    ]\n",
    ")\n",
    "url_data = loaderurl.load()\n",
    "url_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data from Docs uding DocsLoader\n",
    "file = \"./indian.docx\"\n",
    "loaderdocs = UnstructuredWordDocumentLoader(file)\n",
    "doc_data = loaderdocs.load()\n",
    "doc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data from Confluence\n",
    "loader =ConfluenceLoader(\n",
    "    url=\"https://adityas181.atlassian.net/wiki\", username=\"adityas181@gmail.com\", api_key=\"ATATT3xFfGF0enLfG-S2di7ACvuvLpCQXA6fkuyK2ZiHp3ES2sHWsaSvpJVTsf1LPQ_YNif-RZu0A3NhyDnpVAQH3B3LiWxJNOjKHBjEzKxCgwWVeri74H5Hf_Tlpqbs3nQn-tEQesEbRN-ih55TtKmrrEsGz8Ljx3PfcjhCBsAIB8XDi9ye86s=22D8F09F\"\n",
    ")\n",
    "confluence_data = loader.load(space_key=\"documentat\")\n",
    "confluence_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding all data from different sources for spliting and ingestion\n",
    "data = url_data + doc_data + confluence_data \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Spliting the all data using RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=['\\n\\n', '\\n', '.', ','],\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    " \n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can extract the page content and metadata from the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content = [doc.page_content for doc in docs]\n",
    "meta = [doc.metadata for doc in docs]\n",
    "print(len(page_content))\n",
    "print(len(meta))\n",
    "print(page_content)\n",
    "print(meta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loaded the data from different sources.\n",
    "Now importent part is to create a ingestion Pipeline to ingest all data in vector DB\n",
    "Here I am using Opensearch for as vector DB for indexing.\n",
    "Created my Custom embedding class to use our embedding models. I am using openai embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingestion pipeline\n",
    "from embedding import MyEmbedding\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "embeddings = MyEmbedding()  # loading custom embedding class\n",
    "index_name=''  # index name to create. it created automatically by opensearach\n",
    "username = \"\"\n",
    "password = \"\"\n",
    "# creating object of opensearch vector\n",
    "docsearch = OpenSearchVectorSearch(\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings,\n",
    "    opensearch_url=\"\",\n",
    "    http_auth = (username,password)\n",
    ")\n",
    "print(docsearch.index_name)\n",
    "print(docsearch.client.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingestion the Docs that loader from different sources\n",
    "ingest = docsearch.add_documents(documents=docs)\n",
    "print(ingest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing similarity search on thr docs. \n",
    "# using cosinesimil serach type\n",
    "query =\"which are leading authorities for food and drugs?\"\n",
    "#query = \"which are leading authorities for food and drugs?\"\n",
    "#query = \"what are major economic sectors\"\n",
    "docs = docsearch.similarity_search_with_score(query, space_type=\"cosinesimil\")\n",
    "citation = [\n",
    "    {\n",
    "    \"page_content\" :doc[0].page_content,\n",
    "    \"score\" : doc[1]\n",
    "    }\n",
    "    for doc in docs\n",
    "]\n",
    "\n",
    "citation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am making ConversationalRetrievalChain from Langchain\n",
    "Using Stuff method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stuff method\n",
    "from embedding import MyEmbedding\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "openai_key= \"\"\n",
    "embeddings = MyEmbedding()\n",
    "index_name=''\n",
    "username =  ''\n",
    "password = ''\n",
    "vector_db = OpenSearchVectorSearch(\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings,\n",
    "    opensearch_url=\"\",\n",
    "    http_auth = (username,password),\n",
    "    timeout=300,\n",
    "    verbose=True    \n",
    ")\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(ChatOpenAI(openai_api_key=openai_key), chain_type=\"stuff\",\n",
    "                                   retriever=vector_db.as_retriever(),\n",
    "                                   return_source_documents=True)\n",
    "# chain = ConversationalRetrievalChain.from_llm(ChatOpenAI(openai_api_key=openai_key), chain_type=\"stuff\",\n",
    "#                                    retriever=vector_db.as_retriever(search_kwargs={\"k\": 3}))\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking question from document.\n",
    "First it will make embedding of query and then search the embedding in opensearch at particular mebedding then it return the citation and then send citation to LLM and return the Ans of query.\n",
    "HEre we are alos making chat history so it can retain the ans of prevoius question.\n",
    "langchain.debug=True will show each step that langchaon do internally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "query = \"which are leading authorities for food and drugs?\"\n",
    "#query =\"what are major economic sectors\"\n",
    "#query = \"How to use expansion in the REST APIs?\"\n",
    "\n",
    "langchain.debug=True\n",
    "chat_history= []\n",
    "result = chain({\"question\": query,'chat_history': chat_history})\n",
    "ans = result[\"answer\"]\n",
    "citation=result[\"source_documents\"]\n",
    "print(f\" Answer ---{ans}\")\n",
    "print(citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the price of Tiago iCNG?\"\n",
    "#langchain.debug=True\n",
    "chat_history= []\n",
    "result = chain({\"question\": query,'chat_history': chat_history})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we adding the answer of prevoius query and sned angain to LLM. By this it maintain the context of prevous questions.\n",
    "chat_history += [(query, result[\"answer\"])]\n",
    "query = \"can you divide the range by 2\"\n",
    "result = chain({\"question\": query,'chat_history': chat_history}, return_only_outputs=True)\n",
    "print(chat_history)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are using Map reduce method.\n",
    "This method answer the query from accurate but it amke more call to LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## map reduce \n",
    "from embedding import MyEmbedding\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "embeddings = MyEmbedding()\n",
    "index_name=''\n",
    "username =  ''\n",
    "password = ''\n",
    "vector_db = OpenSearchVectorSearch(\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings,\n",
    "    opensearch_url=\"\",\n",
    "    http_auth = (username,password),\n",
    "    timeout=300,\n",
    "    verbose=True\n",
    "    \n",
    ")\n",
    "chain = ConversationalRetrievalChain.from_llm(ChatOpenAI(openai_api_key=openai_key), chain_type=\"map_reduce\",\n",
    "                                   retriever=vector_db.as_retriever())\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"what is the price of Tiago iCNG?\"\n",
    "\n",
    "chat_history= []\n",
    "result = chain({\"question\": query,'chat_history': chat_history})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we adding the answer of prevoius query and sned angain to LLM. By this it maintain the context of prevous questions.\n",
    "chat_history += [(query, result[\"answer\"])]\n",
    "query = \"can you divide the range by 2\"\n",
    "result = chain({\"question\": query,'chat_history': chat_history}, return_only_outputs=True)\n",
    "print(chat_history)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
